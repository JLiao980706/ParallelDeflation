{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stochastic_solvers import eigengame, parallel_deflation\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1000\n",
    "r = 30\n",
    "num_trials = 10\n",
    "batch_size = 1000\n",
    "true_evecs, mat, eig_vals = generate_powerlaw_matrix(d, r, 0.5, return_evals=True)\n",
    "cov_sqrt = true_evecs.T @ np.diag(np.sqrt(eig_vals))\n",
    "data_gen = lambda samples: generate_data_from_cov(cov_sqrt, samples)\n",
    "eval_func = lambda x: compute_avg_error(true_evecs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Deflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.20586997757356515\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.2026491198178617\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.2033828262898145\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.19234544172485796\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.1836551123042344\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.2568868437537148\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.20758197715371088\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.20130790712781307\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.2029457493534233\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.19042447481074562\n"
     ]
    }
   ],
   "source": [
    "L = 600\n",
    "T = 1\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx / 5))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = parallel_deflation(data_gen, r, L, T, step_size_func, batch_size)\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T1 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T1 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T1 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T1 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.42861170093749024\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.18507927784086717\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.2717868528556807\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.191904544556215\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.23120449804311882\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.2161792607451152\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.20247849836915313\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.20039398128958047\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.21509059269219072\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.22283825915913064\n"
     ]
    }
   ],
   "source": [
    "L = 120\n",
    "T = 5\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = parallel_deflation(data_gen, r, L, T, step_size_func, batch_size)\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T5 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T5 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T5 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T5 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "max_err_results = dict()\n",
    "avg_err_results = dict()\n",
    "\n",
    "max_err_results['T=1'] = (max_err_mean_T1.tolist(), max_err_std_T1.tolist())\n",
    "max_err_results['T=5'] = (max_err_mean_T5.tolist(), max_err_std_T5.tolist())\n",
    "\n",
    "avg_err_results['T=1'] = (avg_err_mean_T1.tolist(), avg_err_std_T1.tolist())\n",
    "avg_err_results['T=5'] = (avg_err_mean_T5.tolist(), avg_err_std_T5.tolist())\n",
    "\n",
    "with open('parallel_deflation_powerlaw_sto.txt', 'w+') as jfile:\n",
    "    json.dump(dict(max_result=max_err_results, avg_result=avg_err_results), jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EigenGame-mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.16825698971655856\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.13078993792470525\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.15594383215455182\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.14803389949653523\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.16692851021349275\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.14482608831844163\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.18823390735397413\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.17105566399507\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.14572031179501627\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.174201841617779\n"
     ]
    }
   ],
   "source": [
    "L = 600\n",
    "T = 1\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx / 5))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = eigengame(data_gen, r, L, T, step_size_func, batch_size, update='mu')\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T1 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T1 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T1 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T1 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.37954322221360864\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.3693610633392959\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.1754100677309293\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.18596021945147556\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.26216265134732836\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.21995523389674357\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.16483837055565906\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.17357427192464087\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.16550973999850968\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.1529351874373384\n"
     ]
    }
   ],
   "source": [
    "L = 120\n",
    "T = 5\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = eigengame(data_gen, r, L, T, step_size_func, batch_size, update='mu')\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T5 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T5 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T5 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T5 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "max_err_results = dict()\n",
    "avg_err_results = dict()\n",
    "\n",
    "max_err_results['T=1'] = (max_err_mean_T1.tolist(), max_err_std_T1.tolist())\n",
    "max_err_results['T=5'] = (max_err_mean_T5.tolist(), max_err_std_T5.tolist())\n",
    "\n",
    "avg_err_results['T=1'] = (avg_err_mean_T1.tolist(), avg_err_std_T1.tolist())\n",
    "avg_err_results['T=5'] = (avg_err_mean_T5.tolist(), avg_err_std_T5.tolist())\n",
    "\n",
    "with open('eigengame_mu_powerlaw_sto.txt', 'w+') as jfile:\n",
    "    json.dump(dict(max_result=max_err_results, avg_result=avg_err_results), jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EigenGame-Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.17755927451768536\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.16452613112977318\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.16770138564202455\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.16435209051113386\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.17314284584638973\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.17099192144966283\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.1670065238876101\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.17081596445079167\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.26767050600760256\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.17716426559828513\n"
     ]
    }
   ],
   "source": [
    "L = 600\n",
    "T = 1\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx / 5))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = eigengame(data_gen, r, L, T, step_size_func, batch_size, update='alpha')\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T1 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T1 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T1 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T1 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial #1\n",
      "Trial #1 Error: 0.15506990888468683\n",
      "Running Trial #2\n",
      "Trial #2 Error: 0.16552166399503543\n",
      "Running Trial #3\n",
      "Trial #3 Error: 0.17797404100072942\n",
      "Running Trial #4\n",
      "Trial #4 Error: 0.4174819934136298\n",
      "Running Trial #5\n",
      "Trial #5 Error: 0.16646970116366683\n",
      "Running Trial #6\n",
      "Trial #6 Error: 0.19698560397426398\n",
      "Running Trial #7\n",
      "Trial #7 Error: 0.17525757110203508\n",
      "Running Trial #8\n",
      "Trial #8 Error: 0.3956304244352412\n",
      "Running Trial #9\n",
      "Trial #9 Error: 0.27363015341467944\n",
      "Running Trial #10\n",
      "Trial #10 Error: 0.15212182326500984\n"
     ]
    }
   ],
   "source": [
    "L = 120\n",
    "T = 5\n",
    "step_size_func = lambda idx: 0.2 / (1 + np.floor(idx))\n",
    "\n",
    "all_max_errs = np.zeros((num_trials, L+1))\n",
    "all_avg_errs = np.zeros((num_trials, L+1))\n",
    "for trial_idx in range(num_trials):\n",
    "    print(f'Running Trial #{trial_idx+1}')\n",
    "    evecs_hist = eigengame(data_gen, r, L, T, step_size_func, batch_size, update='alpha')\n",
    "    all_max_errs[trial_idx] = np.array([compute_max_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    all_avg_errs[trial_idx] = np.array([compute_avg_error(true_evecs, evecs) for evecs in evecs_hist])\n",
    "    print(f'Trial #{trial_idx+1} Error: {all_avg_errs[trial_idx,-1]}')\n",
    "\n",
    "max_err_mean_T5 = all_max_errs.mean(axis=0)\n",
    "max_err_std_T5 = all_max_errs.std(axis=0)\n",
    "avg_err_mean_T5 = all_avg_errs.mean(axis=0)\n",
    "avg_err_std_T5 = all_avg_errs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "max_err_results = dict()\n",
    "avg_err_results = dict()\n",
    "\n",
    "max_err_results['T=1'] = (max_err_mean_T1.tolist(), max_err_std_T1.tolist())\n",
    "max_err_results['T=5'] = (max_err_mean_T5.tolist(), max_err_std_T5.tolist())\n",
    "\n",
    "avg_err_results['T=1'] = (avg_err_mean_T1.tolist(), avg_err_std_T1.tolist())\n",
    "avg_err_results['T=5'] = (avg_err_mean_T5.tolist(), avg_err_std_T5.tolist())\n",
    "\n",
    "with open('eigengame_alpha_powerlaw_sto.txt', 'w+') as jfile:\n",
    "    json.dump(dict(max_result=max_err_results, avg_result=avg_err_results), jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
